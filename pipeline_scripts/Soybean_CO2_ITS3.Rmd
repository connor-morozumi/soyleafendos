---
title: "ITS3/4"
output: html_notebook
---

```{r}
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
library(here)
#library(tidyverse)
```

```{r}
path = here("ITS3_ITS4") # CHANGE ME to the directory containing the fastq files after unzipping.
  ## CHANGE ME to the directory containing the fastq files.
list.files(path)

fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))

```

## Identify primers

```{r}
FWD= "GCATCGATGAAGAACGCAGC"  # ITS3
REV= "TCCTCCGCTTATTGATATGC"  # ITS4
```

Check orientation

```{r}

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```
### Prefilter for Ns
The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))

filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

```


### Count primers

We are now ready to count the number of times the primers appear in the forward and reverse read, while considering all possible primer orientations. Identifying and counting the primers on one set of paired end FASTQ files is sufficient, assuming all the files were created using the same library preparation, so we’ll just process the first sample.

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), #just the first file
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```

## Remove Primers

These primers can be now removed using a specialized primer/adapter removal tool. Here, we use `cutadapt` for this purpose. Download, installation and usage instructions are available online: http://cutadapt.readthedocs.io/en/stable/index.html

```{r set up cutadapt files}
cutadapt <- path.expand("~/miniconda3/envs/cutadaptenv/bin/cutadapt")
system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

```

```{r}
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
# for(i in seq_along(fnFs)) {
#   system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
#                              "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
#                              fnFs.filtN[i], fnRs.filtN[i])) # input files
# }


#clara @neonMicrobe does this

# for (i in seq_along(fnFs)) {
#         system2(cutadapt_path, args = c(R1.flags, "-n", 2, "-o", 
#             trimFs[i], fnFs[i], "--minimum-length", "1", discard_untrimmed_flag, 
#             "-e", "0.2"), stdout = ifelse(very_verbose, "", FALSE))
# }

```


As a sanity check, we will count the presence of primers in the first cutadapt-ed sample:
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```

## regular DADA2 pipeline
```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format:
# get.sample.name <- function(fname) strsplit(basename(fname), "-")[[1]][2]
# sample.names <- unname(sapply(cutFs, get.sample.name))
# head(sample.names)

sample.names=gsub("^[^-]*-([^A-Z]+).*", "\\1", cutFs)

sample.names= ifelse(grepl("^\\d+",gsub("^[^-]*-([^a-zA-Z]+)_.*", "\\1", cutFs)), gsub("^[^-]*-([^a-zA-Z]+)_.*", "\\1", cutFs), gsub(".*-(\\w+)(\\_.*)(\\_.*)", "\\1", cutFs))
```

### Inspect read quality profiles
```{r}
# look at quality
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[117])

```

# Filter and trim
Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files.

```{r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

## Filter all
```{r}

# out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), 
#     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
# head(out)
```

# Filter Forward reads only

There is some evidence that poor quality reads associated with reverse sequences lead to poorer recapitulation of mock communities (Pauvert et al.).

With this in mind some have recommended just using the quality forward reads
`filterAndTrim` can be modified to only read in forwards reads
```{r}
out.for <- filterAndTrim(cutFs, filtFs, maxN = 0, maxEE = 2, 
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out.for)

```

# Learn errors
```{r}
#learn error rate
errF <- learnErrors(filtFs, multithread = TRUE)
# errR <- learnErrors(filtRs, multithread = TRUE)
plotErrors(errF, nominalQ = TRUE)
```

These are looking a lot like the ones on the [github issue pages for NovaSeq](https://github.com/benjjneb/dada2/issues/791)

### Learn errors with NovaSeq data
using code found [here:](https://github.com/benjjneb/dada2/issues/1307)

Try #1: alter loess arguments (weights and span) & enforce monotonicity

```{r}
loessErrfun_mod <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF_try1 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod,
  verbose = TRUE
)

plotErrors(errF_try1, nominalQ = TRUE)
```
Try #2:
```{r}
loessErrfun_mod2 <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        # mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}


# check what this looks like
errF2 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod2,
  verbose = TRUE
)

plotErrors(errF2, nominalQ = TRUE)

```
 Try # 3
```{r}
loessErrfun_mod3 <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        # mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        # only change the weights
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot))

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF3 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod3,
  verbose = TRUE
)

plotErrors(errF3, nominalQ = TRUE)

```
 
If these aren't working for you. Someone mentioned this might be due to one (or a handful or unique sequences in the dataset. [comment here](https://github.com/benjjneb/dada2/issues/1307#issuecomment-856980911))

Going to go with the first one somewhat arbitrarily...


### Dereplicate identical reads

```{r}

derepFs <- derepFastq(filtFs, verbose = TRUE)
#derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
#names(derepRs) <- sample.names

```

### Sample inference

```{r}
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
#dadaRs <- dada(derepRs, err = errR, multithread = TRUE)

#getting this warning, not sure where
# Warning in rval[, 1:ncol(tt)] + tt : NAs produced by integer overflow
```


### Construct seq table
```{r}
seqtab.for <- makeSequenceTable(dadaFs)
dim(seqtab.for) # 118 4272 

```
With nova err correction: this time the seq tab is very very different! 118 x 1716!

### Remove chimeras
```{r}
seqtab.nochim.for <- removeBimeraDenovo(seqtab.for, method="consensus", multithread=TRUE, verbose=TRUE) # Identified 2199 bimeras out of 4272 input sequences.

table(nchar(getSequences(seqtab.nochim.for)))


```

### Track reads through

```{r}
getN <- function(x) sum(getUniques(x))

track.for <- cbind(out.for, sapply(dadaFs, getN),  rowSums(seqtab.nochim.for))
colnames(track.for) <- c("input", "filtered.fo", "denoisedF.for", "nonchim.for")
rownames(track.for) <- sample.names
```
### Assign taxonomy
```{r}
unite.ref <- "~/sh_general_release_dynamic_all_10.05.2021.fasta"  # CHANGE ME to location on your machine

```

```{r}
taxa.for <- assignTaxonomy(seqtab.nochim.for , unite.ref, multithread = TRUE, tryRC = TRUE)

```
Inspect
```{r}
taxa.print.for <- taxa.for  # Removing sequence rownames for display only
rownames(taxa.print.for) <- NULL
head(taxa.print.for)

save.dir= here("outputs")

if ( !(dir.exists(save.dir)) ){ 
  dir.create(save.dir)
} 

write.csv(taxa.for, paste0(save.dir, "/taxa.CO2.IT3.",Sys.Date(),".csv"))
# write.csv(taxa.for, "outgroupincluded.taxa.CO2.IT3.csv")

```

### Write ASV tables and outputs

```{r}

write.csv(seqtab.nochim.for, paste0(save.dir, "/seqtab.CO2.IT3.",Sys.Date(),".csv"))
```

Giving our seq headers more manageable names (ASV_1, ASV_2...)

```{r}
asv_seqs.for = colnames(seqtab.nochim.for)
asv_headers.for = vector(dim(seqtab.nochim.for)[2], mode="character")

for (i in 1:dim(seqtab.nochim.for)[2]) {
  asv_headers.for[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta.for = c(rbind(asv_headers.for, asv_seqs.for))
write(asv_fasta.for, paste0(save.dir, "/ASVs.CO2.IT3.",Sys.Date(),".fa"))

# ASV table:
asv_tab.for = t(seqtab.nochim.for)
row.names(asv_tab.for) = sub(">", "", asv_headers.for)
asv_tab.for = t(asv_tab.for)
write.table(asv_tab.for, paste0(save.dir, "/ASVs_counts.CO2.IT3",Sys.Date(),".tsv"), sep="\t", quote=F, col.names=NA)
```

Session info and saving workspace

Since these processes are computationally intensive and not instantaneous it might be useful to reload a workspace 
```{r}
# save.image() #saves an .RData to the base folder (in this case to the CO2_Project folder). Note these can get really big so might not be worth it 

#good practice for reproducible sci to output Session Information
sessionInfo()
```
