---
title: "CO2 Archaea"
output: html_notebook
---
# Packages and setup
```{r}
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
library(here)
#library(tidyverse)
```

Now we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.
```{r}
path = here("CO2_Project","Arch349F_Arch806R") # CHANGE ME to the directory containing the fastq files after unzipping.
  ## CHANGE ME to the directory containing the fastq files.
list.files(path)

fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
      
```


```{r}
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
# dadd2 tutorial example
# sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# Extract sample names, assuming filenames have format:
# get.sample.name <- function(fname) strsplit(basename(fname), "-")[[1]][2]
# sample.names <- unname(sapply(cutFs, get.sample.name))
# head(sample.names)

sample.names= ifelse(grepl("^\\d+",gsub("^[^-]*-([^a-zA-Z]+)_.*", "\\1", fnFs)), gsub("^[^-]*-([^a-zA-Z]+)_.*", "\\1", fnFs), gsub(".*-(\\w+)(\\_.*)(\\_.*)", "\\1", fnFs))
```

# Preprocessing with `fastp`

We have to deal with some NovaSeq polyG tail issues (and hopefully the polyCs get trimmed off as well)

Set up `fastp` and get some files copied over to a few folder called `fastpFilteredReads`
```{r}

fastp <- path.expand("~/miniconda3/envs/fastpenv/bin/fastp")

# hello, is this thing on?
system2(fastp, "-h")

#make a fastp filtered folder
path.fastp <- file.path(path, "fastpFilteredReads")
if(!dir.exists(path.fastp)) dir.create(path.fastp)

fastpFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fastpRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))

fastpF.trim <- file.path(path, "fastpFilteredReads", basename(fastpFs)) # Put fastp filtered reads in fastp subdirectory
fastpR.trim <- file.path(path, "fastpFilteredReads", basename(fastpRs)) 

# dev to delete: test loop on just a handful

#make a TEST fastp filtered folder
test.fastp <- file.path(path, "test")
if(!dir.exists(test.fastp)) dir.create(test.fastp)
# copy in first 3 manually because lazy
testfastpFs <- sort(list.files(test.fastp, pattern = "_R1.fastq", full.names = TRUE))
testfastpRs <- sort(list.files(test.fastp, pattern = "_R2.fastq", full.names = TRUE))

test.trim.fastp <- file.path(test.fastp, "trim")
if(!dir.exists(test.trim.fastp)) dir.create(test.trim.fastp)

fnFs.trim <- file.path(test.trim.fastp, basename(testfastpFs))
fnRs.trim  <- file.path(test.trim.fastp, basename(testfastpRs))

#seems to work! seqs coming out seem reasonable
for(i in seq_along(testfastpFs)){
  message('Processing file ', testfastpFs[i], ' of ', length(testfastpFs))
  message('And R2 file ', testfastpRs[i], ' of ', length(testfastpFs))

  #Run fastp
  system2(fastp, args= c("--in1" , testfastpFs[i],"--in2" , testfastpRs[i], "--out1", fnFs.trim[i],"--out2", fnRs.trim[i], "-l", 50, "-h", "arch.html", "--trim_poly_g", "&>", "test.log"))
}
```

Run fastp
```{r}
# test run on a single fastq 
system2(fastp, args= c("--in1" ,"~/CO2_Project/Arch349F_Arch806R/fastpFilteredReads/Arch349F_Arch806R-21_15_CGTAATGAGC_R1.fastq","--out1", "~/CO2_Project/Arch349F_Arch806R/fastpFilteredReads/Arch349F_Arch806R-21_15_CGTAATGAGC_R1.trimmed.fastq", "-l", 50, "-h", "arch.html", "--trim_poly_g", "&>", "test.log"))

# for loop over all files
for(i in seq_along(fastpFs)){
  #Run fastp
  system2(fastp, args= c("--in1" , fastpFs[i],"--in2" , fastpRs[i], "--out1", fastpF.trim[i],"--out2", fastpR.trim[i], "-l", 50, "-h", "arch.html", "--trim_poly_g", "&>", "test.log"))
}

#I don't know why this isn't working, it's doing inconsistent things each time I try it.
# weirdly seems to be not doing the first i? 
# and many are now coming back at 0kb totally empty files... hmmmm
# IDK maybe try splitting in half and running a smaller batch next
```

# Inspect read quality profiles
We start by visualizing the quality profiles of the forward reads:
```{r}
plotQualityProfile(fnFs[1:3])

plotQualityProfile(fnRs[42:46])
```

Unsure if these are any good. I think the bands are coming from the binned quality scores but not sure.

> Considerations for your own data: Your reads must still overlap after truncation in order to merge them later! The tutorial is using 2x250 V4 sequence data, so the forward and reverse reads almost completely overlap and our trimming can be completely guided by the quality scores. If you are using a less-overlapping primer set, like V1-V2 or V3-V4, your truncLen must be large enough to maintain 20 + biological.length.variation nucleotides of overlap between them.

> Once you've identified the length of the sequenced amplicon (which may or may not include primers), you have to make sure that your truncation lengths add up to the sequenced amlpicon length plus an overlap margin of say 20 nts or more.
From there, select truncation lengths that are long enough to overlap, and that cut off as much low quality tail as you can.
 
For this primer region: 
Expected length(nt) of amplified region including locus-specific primer sequences: 457
So if we truncated R at 240 and left F at 0 = 490- 20 (buffer)= 470 


# Filter and trim
Assign the filenames for the filtered fastq.gz files.

# Place filtered files in filtered/ subdirectory

```{r}
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

In the standard 16S workflow, it is generally possible to remove primers (when included on the reads) via the `trimLeft` parameter `(filterAndTrim(..., trimLeft=(FWD_PRIMER_LEN, REV_PRIMER_LEN)))`
```{r}
FWD= 'GYGCASCAGKCGMGAAW'
REV= 'GGACTACVSGGGTATCTAAT'

FWD_PRIMER_LEN = nchar(FWD)    
REV_PRIMER_LEN = nchar(REV)  
```

We’ll use standard filtering parameters: maxN=0 (DADA2 requires no Ns), truncQ=2, rm.phix=TRUE and maxEE=2. The maxEE parameter sets the maximum number of “expected errors” allowed in a read, which is a better filter than simply averaging quality scores.

- we didn't remove primers before the run so we also add the ` trimLeft=(FWD_PRIMER_LEN, REV_PRIMER_LEN)` argument
-  If both truncLen and trimLeft are provided, filtered reads will have length truncLen-trimLeft.

```{r}
out.stand <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(0,240),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE,
               trimLeft=(c(FWD_PRIMER_LEN, REV_PRIMER_LEN))) # trim primers off based on length of primer (ok to do with 16s)
 # On Windows set multithread=FALSE

out.polG<- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(0,240),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE,
               trimLeft=(c(FWD_PRIMER_LEN, REV_PRIMER_LEN)),
              rm.lowcomplex = 8)
head(out)

out.type= out.polG
```

# Learn the Error Rates

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)

errormod= errF
errormodR= errR
```

Still having those binning error rate issues

### Learn errors with NovaSeq data
using code found [here:](https://github.com/benjjneb/dada2/issues/1307)

Try #1: alter loess arguments (weights and span) & enforce monotonicity

```{r}
loessErrfun_mod <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF_try1 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod,
  verbose = TRUE
)

plotErrors(errF_try1, nominalQ = TRUE)

errormod= errF_try1

errormodname= "errF1"
```

# Sample Inference
```{r}
dadaFs <- dada(filtFs, err = errormod, multithread=TRUE)
dadaRs <- dada(filtRs, err = errormodR, multithread=TRUE)
```

> Extensions: By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. The dada2 package offers two types of pooling. dada(..., pool=TRUE) performs standard pooled processing, in which all samples are pooled together for sample inference. dada(..., pool="pseudo") performs pseudo-pooling, in which samples are processed independently after sharing information between samples, approximating pooled sample inference in linear time.
 
# Merge paired reads
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[2]])
```

# Construct sequence table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```


Sequencing center gave us this target for total fragment length (Expected total fragment length(nt)
 with additional index and adaptors): 570

and said the Expected length(nt) of amplified region
 including locus-specific primer sequences would be: 457
457-20

hmm will have to look into this more


# Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

#proportion chimera
sum(seqtab.nochim)/sum(seqtab)
```

#Track reads through the pipeline
As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out.type, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)

as.data.frame(track) %>% 
  rownames_to_column("sample") %>% 
  pivot_longer(! sample , names_to = "step", values_to = "count") %>% 
  mutate(name = fct_relevel(step, 
            "input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")) %>%
ggplot(., aes(x= name, y= count, color= sample, group= sample)) +
  geom_point() +
  geom_line(aes(color= sample))+
  theme(legend.position = "none") +
  labs(y= "Number of reads")
```

# Assign taxonomy
```{r}
taxa <- assignTaxonomy(seqtab.nochim, "~/silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)
```

Extensions: The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. Recent analysis suggests that exact matching (or 100% identity) is the only appropriate way to assign species to 16S gene fragments. Currently, species-assignment training fastas are available for the Silva and RDP 16S databases. To follow the optional species addition step, download the silva_species_assignment_v132.fa.gz file, and place it in the directory with the fastq files.

NOTE: These database files have a known problem in 3/895 families and 59/3936 genera. See https://github.com/mikemc/dada2-reference-databases/blob/main/silva-138.1/v1/bad-taxa.csv for a list of affected taxa and https://github.com/benjjneb/dada2/issues/1293 for more information.

```{r}
taxa <- addSpecies(taxa, "~/silva_species_assignment_v138.1.fa.gz")
```

Inspect

We have several seqs that look weird 
```{r}
rownames(taxa)[33]
```

```{r}
taxa.print <- taxa  # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```

### Write ASV table
Giving our seq headers more manageable names (ASV_1, ASV_2...)

```{r}
write.csv(taxa, "taxa.CO2.archaea.csv")

asv_seqs = colnames(seqtab.nochim)
asv_headers= vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta= c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.archaea.fa")

# ASV table:
asv_tab = t(seqtab.nochim)
row.names(asv_tab) = sub(">", "", asv_headers)
asv_tab= t(asv_tab)
write.table(asv_tab, "ASVs_counts.archaea.tsv", sep="\t", quote=F, col.names=NA)
```

```{r}
save.image(paste0(path, "/archaea.RData")) #saves an .RData to the base folder (in this case to the V4_515F... folder). Note these can get really big so might not be worth it 
```


