---
title: "R Notebook"
output: html_notebook
---

```{r}
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
library(here)
library(tidyverse)
```


```{r}
path = here("ITS1F_ITS2") # CHANGE ME to the directory containing the fastq files after unzipping.
  ## CHANGE ME to the directory containing the fastq files.
list.files(path)

fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))

```

## Identify primers

```{r}
FWD= "CTTGGTCATTTAGAGGAAGTAA"
REV= "GCTGCGTTCTTCATCGATGC"
```

Check orientation

```{r}

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```
### Prefilter for Ns
The presence of ambiguous bases (Ns) in the sequencing reads makes accurate mapping of short primer sequences difficult. Next we are going to “pre-filter” the sequences just to remove those with Ns, but perform no other filtering.

```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
```

### Count primers

We are now ready to count the number of times the primers appear in the forward and reverse read, while considering all possible primer orientations. Identifying and counting the primers on one set of paired end FASTQ files is sufficient, assuming all the files were created using the same library preparation, so we’ll just process the first sample.

```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), #just the first file
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```
## Remove Primers

These primers can be now removed using a specialized primer/adapter removal tool. Here, we use `cutadapt` for this purpose. Download, installation and usage instructions are available online: http://cutadapt.readthedocs.io/en/stable/index.html

```{r set up cutadapt files}
cutadapt <- path.expand("~/miniconda3/envs/cutadaptenv/bin/cutadapt")
system2(cutadapt, args = "--version") # Run shell commands from R

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))

```

```{r}
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}
```

As a sanity check, we will count the presence of primers in the first cutadapt-ed sample:
```{r}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
```

## regular DADA2 pipeline
```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1.fastq", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format:
# get.sample.name <- function(fname) strsplit(basename(fname), "-")[[1]][2]
# sample.names <- unname(sapply(cutFs, get.sample.name))
# head(sample.names)

# sample.names=gsub("^[^-]*-([^A-Z]+).*", "\\1", cutFs)

sample.names= ifelse(grepl("^\\d+",gsub("^[^-]*-([^a-zA-Z]+)_.*", "\\1", cutFs)), gsub("^[^-]*-([^a-zA-Z]+)_.*", "\\1", cutFs), gsub(".*-(\\w+)(\\_.*)(\\_.*)", "\\1", cutFs))
```

### Inspect read quality profiles
```{r}
# look at quality
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[117])

```

# Filter and trim
Assigning the filenames for the output of the filtered reads to be stored as fastq.gz files.

```{r}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))
```

```{r}

# out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), 
#     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
# head(out)
```
## Forward reads only

There is some evidence that poor quality reads associated with reverse sequences lead to poorer recapitulation of mock communities (Pauvert et al.).

With this in mind some have recommended just using the quality forward reads
`filterAndTrim` can be modified to only read in forwards reads
```{r}
# a note we already have these outputs when we run both for and rev on lines 148-153
out.for <- filterAndTrim(cutFs, filtFs, maxN = 0, maxEE = 2, 
    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out.for)

```

# Learn errors
```{r}
#learn error rate
errF <- learnErrors(filtFs, multithread = TRUE)
# errR <- learnErrors(filtRs, multithread = TRUE)
plotErrors(errF, nominalQ = TRUE)

errormod= errF 

errormodname= "errFDefault"
```

These are looking a lot like the ones on the [github issue pages for NovaSeq](https://github.com/benjjneb/dada2/issues/791)

### Learn errors with NovaSeq data
using code found [here:](https://github.com/benjjneb/dada2/issues/1307)

Try #1: alter loess arguments (weights and span) & enforce monotonicity

```{r}
loessErrfun_mod <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF_try1 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod,
  verbose = TRUE
)

plotErrors(errF_try1, nominalQ = TRUE)

errormod= errF_try1

errormodname= "errF1"
```
Try #2:
```{r}
loessErrfun_mod2 <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        # mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}


# check what this looks like
errF2 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod2,
  verbose = TRUE
)

plotErrors(errF2, nominalQ = TRUE)

errormod= errF2
errormodname= "errF2"
```
 Try # 3
```{r}
loessErrfun_mod3 <- function(trans) {
  qq <- as.numeric(colnames(trans))
  est <- matrix(0, nrow=0, ncol=length(qq))
  for(nti in c("A","C","G","T")) {
    for(ntj in c("A","C","G","T")) {
      if(nti != ntj) {
        errs <- trans[paste0(nti,"2",ntj),]
        tot <- colSums(trans[paste0(nti,"2",c("A","C","G","T")),])
        rlogp <- log10((errs+1)/tot)  # 1 psuedocount for each err, but if tot=0 will give NA
        rlogp[is.infinite(rlogp)] <- NA
        df <- data.frame(q=qq, errs=errs, tot=tot, rlogp=rlogp)

        # original
        # ###! mod.lo <- loess(rlogp ~ q, df, weights=errs) ###!
        # mod.lo <- loess(rlogp ~ q, df, weights=tot) ###!
        # #        mod.lo <- loess(rlogp ~ q, df)

        # Gulliem Salazar's solution
        # https://github.com/benjjneb/dada2/issues/938
        # mod.lo <- loess(rlogp ~ q, df, weights = log10(tot),span = 2)

        # only change the weights
        mod.lo <- loess(rlogp ~ q, df, weights = log10(tot))

        pred <- predict(mod.lo, qq)
        maxrli <- max(which(!is.na(pred)))
        minrli <- min(which(!is.na(pred)))
        pred[seq_along(pred)>maxrli] <- pred[[maxrli]]
        pred[seq_along(pred)<minrli] <- pred[[minrli]]
        est <- rbind(est, 10^pred)
      } # if(nti != ntj)
    } # for(ntj in c("A","C","G","T"))
  } # for(nti in c("A","C","G","T"))

  # HACKY
  MAX_ERROR_RATE <- 0.25
  MIN_ERROR_RATE <- 1e-7
  est[est>MAX_ERROR_RATE] <- MAX_ERROR_RATE
  est[est<MIN_ERROR_RATE] <- MIN_ERROR_RATE

  # enforce monotonicity
  # https://github.com/benjjneb/dada2/issues/791
  estorig <- est
  est <- est %>%
    data.frame() %>%
    mutate_all(funs(case_when(. < X40 ~ X40,
                              . >= X40 ~ .))) %>% as.matrix()
  rownames(est) <- rownames(estorig)
  colnames(est) <- colnames(estorig)

  # Expand the err matrix with the self-transition probs
  err <- rbind(1-colSums(est[1:3,]), est[1:3,],
               est[4,], 1-colSums(est[4:6,]), est[5:6,],
               est[7:8,], 1-colSums(est[7:9,]), est[9,],
               est[10:12,], 1-colSums(est[10:12,]))
  rownames(err) <- paste0(rep(c("A","C","G","T"), each=4), "2", c("A","C","G","T"))
  colnames(err) <- colnames(trans)
  # Return
  return(err)
}

# check what this looks like
errF3 <- learnErrors(
  filtFs,
  multithread = TRUE,
  errorEstimationFunction = loessErrfun_mod3,
  verbose = TRUE
)

plotErrors(errF3, nominalQ = TRUE)

errormod= errF3
errormodname= "errF3"
```
 
If these aren't working for you. Someone mentioned this might be due to one (or a handful or unique sequences in the dataset. [comment here](https://github.com/benjjneb/dada2/issues/1307#issuecomment-856980911))

Going to go with the first one somewhat arbitrarily...


## Dereplicate identical reads

```{r}

derepFs <- derepFastq(filtFs, verbose = TRUE)
#derepRs <- derepFastq(filtRs, verbose = TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
#names(derepRs) <- sample.names

```

## Sample inference

```{r}
dadaFs <- dada(derepFs, err = errormod, multithread = TRUE)
#dadaRs <- dada(derepRs, err = errR, multithread = TRUE)


```


## Construct Sequence Table
We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}
seqtab.for <- makeSequenceTable(dadaFs)
dim(seqtab.for) # 118 570 

```

## Remove chimeras
```{r}
seqtab.nochim.for <- removeBimeraDenovo(seqtab.for, method="consensus", multithread=TRUE, verbose=TRUE) # Identified 52 bimeras out of 625 input sequences.



table(nchar(getSequences(seqtab.nochim.for)))

```
11.7: Using default err: Identified 45 bimeras out of 619 input sequences
Using err_1: Identified 52 bimeras out of 625 input sequences.

### Track reads through

```{r}
getN <- function(x) sum(getUniques(x))
track.for <- cbind(out.for, sapply(dadaFs, getN),  rowSums(seqtab.nochim.for))
colnames(track.for) <- c("input", "filtered", "denoisedF", "nonchim")
rownames(track.for) <- sample.names

as.data.frame(track.for) %>% 
  rownames_to_column("sample") %>% 
  pivot_longer(! sample , names_to = "step", values_to = "count") %>% 
  mutate(name = fct_relevel(step, 
            "input", "filtered", "denoisedF", "nonchim")) %>%
ggplot(., aes(x= name, y= count, color= sample, group= sample)) +
  geom_point() +
  geom_line(aes(color= sample))+
  theme(legend.position = "none") +
  labs(y= "Number of reads")
```

Sample 29_4 loses lots upon filtering step

### Assign taxonomy
Using the Unite database with Euk included
```{r}
unite.ref <- "~/sh_general_release_dynamic_all_10.05.2021.fasta"  # CHANGE ME to location on your machine
taxa.for <- assignTaxonomy(seqtab.nochim.for, unite.ref, multithread = TRUE, tryRC = TRUE)

# unite.ref.noout <- "~/sh_general_release_dynamic_10.05.2021.fasta"
# taxa.nooutgroup = assignTaxonomy(seqtab.nochim.for, unite.ref.noout, multithread = TRUE, tryRC = TRUE)
```
Inspect
```{r}
taxa.print.for <- taxa.for  # Removing sequence rownames for display only
rownames(taxa.print.for) <- NULL
head(taxa.print.for)
```

### Write ASV table and taxa CSV

Make an outputs file
```{r}

output= here("outputs")
if(!(dir.exists(output))){dir.create(output)}
```

Giving our seq headers more manageable names (ASV_1, ASV_2...)
```{r}
asv_seqs.for = colnames(seqtab.nochim.for)
asv_headers.for = vector(dim(seqtab.nochim.for)[2], mode="character")

for (i in 1:dim(seqtab.nochim.for)[2]) {
  asv_headers.for[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta.for = c(rbind(asv_headers.for, asv_seqs.for))
write(asv_fasta.for, paste0(output, "/",errormodname ,".ASVs.ITS1.",Sys.Date(),".fa"))

# ASV table:
asv_tab.for = t(seqtab.nochim.for)
row.names(asv_tab.for) = sub(">", "", asv_headers.for)
asv_tab.for = t(asv_tab.for)

write.table(asv_tab.for, paste0(output, "/",errormodname , ".ASVs_counts.ITS1.",Sys.Date(), ".tsv"), sep="\t", quote=F, col.names=NA)
```

Write taxa csv
```{r}
write.csv(taxa.for, paste0(output, "/", errormodname , ".taxa.CO2.ITS1.",Sys.Date(), ".csv"))

# write.csv(taxa.nooutgroup, paste0(output, "/nooutgroup.taxa.CO2.ITS1.csv"))
```

# Phyloseq

```{r}
# library(phyloseq); packageVersion("phyloseq")
# 
# meta=read.csv("DNA Extractions 2020 Christian SOYface.csv", header = T)
# names(meta)[1] <- 'sample_name'
# meta= meta[,1:2]
# meta=meta %>% 
# column_to_rownames(sample_name)
# 
# seqtab.nochim_no_control= seqtab.nochim[-118,] 
# 
# ps <- phyloseq(otu_table(seqtab.nochim_no_control, taxa_are_rows=FALSE), 
#                sample_data(meta), 
#                tax_table(taxa))
# ps   
# plot_richness(ps, x="Time", measures=c("Shannon", "Simpson"), color="Plant")
# GP.ord <- ordinate(ps, "NMDS", "bray")
# p2 = plot_ordination(ps, GP.ord, type="samples", color="Time", shape="Time") 
# p2
```


# removed sections

This used to occur directly after sample inference set: `dada` and before constructing sequence table ASV

### Merge paired reads
```{r}
# mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
```

# Session info and saving workspace

Since these processes are computationally intensive and not instantaneous it might be useful to reload a workspace 
```{r}
save.image(paste0(path, "/ITS1.RData")) #saves an .RData to the base folder (in this case to the CO2_Project folder). Note these can get really big so might not be worth it 

#good practice for reproducible sci to output Session Information
sessionInfo()
```

